{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHADrrUvXG2n"
      },
      "source": [
        "# Title: AIDI 1002 Final Term Project Report\n",
        "\n",
        "#### Pabhat Kumar Singh: 200545457@student.georgianc.on.ca\n",
        "#### Nishant Kumar: 200545934@student.georgianc.on.ca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvvVdiCEXG2z"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "#### Problem Description:\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers, or BERT, is a well-liked pre-training technique for problems involving natural language processing that has produced noteworthy results lately. Although BERT has proved successful in a variety of NLP activities, there are a number of restrictions that may limit how well it performs in some NLP tasks. These drawbacks include a fixed-length attention mechanism, a lack of attentional specificity, a failure to explicitly model relationships between various input modalities, and a high number of parameters that make it computationally expensive to train and use in environments with limited resources.\n",
        "\n",
        "\n",
        "#### Context of the Problem:\n",
        "\n",
        "The desire for natural language processing models that can efficiently and accurately carry out a variety of language-related tasks, such as sentiment analysis, question-answering, and language translation, is driving the challenge with BERT. BERT was developed as a pre-training technique for problems involving natural language processing, and it has produced outstanding performance on numerous benchmarks. Nevertheless, despite its success, BERT has drawbacks that may reduce its efficiency in some situations. The drawbacks include a fixed-length attention mechanism, a lack of attentional specificity, a failure to explicitly model relationships between various input modalities, and a high number of parameters that make it computationally expensive to train and use in environments with limited resources. \n",
        "\n",
        "#### Limitation About other Approaches:\n",
        "\n",
        "The following succinctly describes the BERT approach's shortcomings in natural language processing:\n",
        "\n",
        "-> Limited attention span: Because BERT's attention mechanism can only focus on a certain number of tokens in the input sequence, it can be difficult to detect long-range dependencies.\n",
        "\n",
        "-> Lack of specificity: BERT's attention mechanism does not explicitly distinguish between various aspects of the input, such as the subject and object in a sentence or the sentiment and content of a text, which can restrict its capacity to capture fine-grained information that is crucial for some tasks.\n",
        "\n",
        "-> Inability to model relationships between different modalities: BERT lacks the capacity to explicitly represent relationships between various input modalities, such as text and images, which makes it difficult for the model to tackle tasks that call for a knowledge of links between various modalities.\n",
        "\n",
        "-> Computationally expensive: Costly to train and deploy: BERT is a complex model with numerous parameters, making it costly to do so, especially in contexts with limited resources.\n",
        "\n",
        "-> Limited transferability: Although BERT has performed well in many NLP tasks, it may not translate well to tasks that are unrelated to its pre-training goals, necessitating further fine-tuning or retraining on task-specific data.\n",
        "\n",
        "#### Solution:\n",
        "\n",
        "A cutting-edge natural language processing model called DEBERTA (Decoding-Enhanced BERT with Disentangled Attention) was released by Microsoft Research in 2020. It is a development of the well-known BERT (Bidirectional Encoder Representations from Transformers) model, which has achieved outstanding outcomes in a variety of NLP tasks. To boost BERT's performance, DEBERTA combines a number of cutting-edge characteristics, such as disentangled attention and enhanced decoding.\n",
        "\n",
        "The model may selectively pay attention to various parts of the input using the new method of disentangled attention, such as the subject and object of a sentence or the sentiment and content of a document. This makes it possible for the model to gather more precise data and enhances its capacity to manage challenging natural language understanding jobs.\n",
        "\n",
        "Another significant element of DEBERTA is enhanced decoding, which includes building more layers into the model to help it manage long-range dependencies in the input. In tasks like language generation and text completion, this enables the model to provide output that is more accurate and coherent.\n",
        "\n",
        "DEBERTA has outperformed the original BERT model in many ways and has produced cutting-edge outcomes on a variety of NLP tasks, including text categorization, question answering, and natural language inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AcF-iaKXG20"
      },
      "source": [
        "# Background\n",
        "\n",
        "Explain the related work using the following table\n",
        "\n",
        "| Reference |Explanation |  Dataset/Input |Weakness\n",
        "| --- | --- | --- | --- |\n",
        "| Devlin et al. [1] | The BERT model, which pre-trains a deep bidirectional transformer on a huge quantity of text data to enhance performance on subsequent natural language processing tasks, is introduced in this study.| the BooksCorpus and the English Wikipedia. | Due to the fact that the datasets are mostly composed of written material, they may not be representative of all linguistic contexts and modes.\n",
        "| Yang et al. [2] | An autoregressive method with a permutation-based training target is used by the pre-training language model \"XLNet\" to identify long-term dependencies in text data.| Wikipedia and BookCorpus | The BookCorpus dataset only includes English-language literature, which restricts the model's capacity to be applied to other languages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0YSxBlYXG21"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "By adding a disentangled attention mechanism, which enables the model to better capture long-range relationships and fine-grained information in the input, the DeBERTa model enhances the original BERT technique. In particular, the DeBERTa model substitutes a two-stage attention process for the conventional multi-head attention mechanism utilised in BERT.\n",
        "\n",
        "For each input token, the model calculates a collection of key and value vectors. The semantic characteristics of these vectors, such as the part of speech or the sentiment of the token, are then used to divide them into several groups. For each token, the model computes a set of query vectors in the second stage. Based on the groups of key and value vectors' semantic features, the model then employs a disentangled attention mechanism to focus on the relevant groups of vectors.\n",
        "\n",
        "The DeBERTa model is able to attend to certain components of the input more explicitly and collect fine-grained information, which may be crucial for some NLP tasks, by detaching the attention mechanism in this way. A mask-predicting training objective and a task-specific adaptor layer are two more improvements the model makes that further boost its performance.\n",
        "\n",
        "The DeBERTa model performs at the cutting edge on many of these tasks when trained and tested on benchmark datasets like GLUE, SuperGLUE, and SQuAD."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have imnplemented Kfold in DeBERTa Model to increse the efficiency"
      ],
      "metadata": {
        "id": "tnAdMXTc9MWy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RB4RnIUXG21"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6d4f47d8edaf44cf9c93259e2367de83",
            "155b7e730ef74413994c672dcb210904",
            "c722c185bc1647928b745127fc880b35"
          ]
        },
        "id": "MYd7BaN7XG21",
        "outputId": "a96991f2-5604-41bd-f8d5-805482ac4a2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d4f47d8edaf44cf9c93259e2367de83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "155b7e730ef74413994c672dcb210904",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c722c185bc1647928b745127fc880b35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#imports\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbPTTcpSXG23",
        "outputId": "d67bebf3-0756-46ca-9fd1-5db32b60ddff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_Shape: (3911, 8)\n",
            "Test_Shape: (3, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id                                          full_text  cohesion  \\\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
              "\n",
              "   syntax  vocabulary  phraseology  grammar  conventions  \n",
              "0     3.5         3.0          3.0      4.0          3.0  \n",
              "1     2.5         3.0          2.0      2.0          2.5  \n",
              "2     3.5         3.0          3.0      3.0          2.5  \n",
              "3     4.5         4.5          4.5      4.0          5.0  \n",
              "4     3.0         3.0          3.0      2.5          2.5  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#loading train and test dataset\n",
        "train = pd.read_csv(r\"D:\\prabhat\\assignment\\ml\\kaggle\\feedback-prize-english-language-learning/train.csv\")\n",
        "print(f'Train_Shape: {train.shape}')\n",
        "test=pd.read_csv(r\"D:\\prabhat\\assignment\\ml\\kaggle\\feedback-prize-english-language-learning/test.csv\")\n",
        "print(f'Test_Shape: {test.shape}')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU26eh2pXG24",
        "outputId": "d27a15a8-8d92-41ca-d73c-8a3d44608538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample_Shape: (3, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000C359D63E</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000BAD50D026</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00367BB2546B</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  \\\n",
              "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0   \n",
              "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0   \n",
              "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0   \n",
              "\n",
              "   conventions  \n",
              "0          3.0  \n",
              "1          3.0  \n",
              "2          3.0  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample = pd.read_csv(r'D:\\prabhat\\assignment\\ml\\kaggle\\feedback-prize-english-language-learning/sample_submission.csv')\n",
        "print(f'Sample_Shape: {sample.shape}')\n",
        "sample.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezx1B3nfXG24",
        "outputId": "8084d3b0-860b-447b-c76e-e5dbba06279f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_Shape: 3519\n",
            "Val_Shape: 196\n",
            "Test_Shape: 196\n"
          ]
        }
      ],
      "source": [
        "# train_test_split\n",
        "np.random.seed(42)\n",
        "df_train, df_val, df_test = np.split(train.sample(frac=1, random_state=42), [int(.9*len(train)), int(.95*len(train))])\n",
        "\n",
        "print(f'Train_Shape: {len(df_train)}')\n",
        "print(f'Val_Shape: {len(df_val)}')\n",
        "print(f'Test_Shape: {len(df_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE4v81QUXG25",
        "outputId": "5369ee67-11ce-4ec2-8b70-08dd869cfdda"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>i think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>when a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>dear, principal  if u change the school policy...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>the best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        text_id                                          full_text  cohesion  \\\n",
              "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
              "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
              "2  00299B378633  dear, principal  if u change the school policy...       3.0   \n",
              "3  003885A45F42  the best time in life is when you become yours...       4.5   \n",
              "4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n",
              "\n",
              "   syntax  vocabulary  phraseology  grammar  conventions  \n",
              "0     3.5         3.0          3.0      4.0          3.0  \n",
              "1     2.5         3.0          2.0      2.0          2.5  \n",
              "2     3.5         3.0          3.0      3.0          2.5  \n",
              "3     4.5         4.5          4.5      4.0          5.0  \n",
              "4     3.0         3.0          3.0      2.5          2.5  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#clean the training dataset by removing space from the text and make the letters in lowercase\n",
        "def clean_text(data):\n",
        "    data = data.copy()\n",
        "    data['full_text'] = data['full_text'].apply(lambda x : x.replace('\\n', ' '))\n",
        "    data['full_text'] = data['full_text'].apply(lambda x: x.strip())\n",
        "    data['full_text'] = data['full_text'].apply(lambda x: x.lower())\n",
        "    return data \n",
        "#applying the text\n",
        "train_df = clean_text(train)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovq2rk3uXG25"
      },
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('../input/huggingface-bert-variants/bert-large-cased/bert-large-cased')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]].reset_index()\n",
        "        self.texts = df[[\"full_text\"]].reset_index()\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels.loc[idx].values[1:]).astype(float)\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return tokenizer(self.texts.loc[idx].values[1], \n",
        "                        padding='max_length', max_length = 512, truncation=True,\n",
        "                        return_tensors=\"pt\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ70fyaoXG25"
      },
      "outputs": [],
      "source": [
        "#Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrdI1dGdXG26"
      },
      "outputs": [],
      "source": [
        "class TestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        \n",
        "        self.texts = df[[\"full_text\"]].reset_index()\n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.texts)\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return tokenizer(self.texts.loc[idx].values[1], padding = 'max_length', max_length = 512, truncation = True, return_tensors = 'pt')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        \n",
        "        return batch_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFOZbaOOXG26"
      },
      "outputs": [],
      "source": [
        "class FeedbackModel(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout = 0.1): \n",
        "\n",
        "        super(FeedbackModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-large-cased\")\n",
        "\n",
        "#         self.bert = BertModel.from_pretrained('../input/huggingface-bert-variants/bert-large-cased/bert-large-cased')\n",
        "        self.dropout = nn.Dropout(dropout) # nn.Dropout(dropout,0)\n",
        "        self.linear = nn.Linear(1024, 256)\n",
        "        self.relu = nn.ReLU() # nn.LeakyReLU(0.1)\n",
        "        self.out = nn.Linear(256,6)\n",
        "        \n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, x = self.bert(input_ids = input_id, attention_mask = mask, return_dict = False)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        final_layer = self.out(x)\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKW1nseSXG26"
      },
      "outputs": [],
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQZys-1sXG27"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, val_data, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "    if torch.cuda.is_available():  \n",
        "        dev = \"cuda:0\" \n",
        "    else:  \n",
        "        dev = \"cpu\"  \n",
        "    device = torch.device(dev) \n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = Adam(model.parameters(), lr = 1e-5)\n",
        "    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 500, eta_min = 1e-6)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size = 2, shuffle = True) \n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size = 2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_labels in tqdm(train_dataloader):\n",
        "\n",
        "                train_labels = train_labels.to(device).float()\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                batch_loss = criterion(output, train_labels)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "            \n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                                \n",
        "            print(f'Epoch: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju1h8W4FXG27"
      },
      "outputs": [],
      "source": [
        "#predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgpJR6OkXG27"
      },
      "outputs": [],
      "source": [
        "def predict(model, test_data):\n",
        "    \n",
        "    test =  TestDataset(test_data)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test,batch_size = 1)\n",
        "    \n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    if use_cuda:\n",
        "        \n",
        "        model = model.cuda()\n",
        "        \n",
        "    out = []\n",
        "    with torch.no_grad():\n",
        "        for test_input in tqdm(test_dataloader):\n",
        "            mask = test_input[\"attention_mask\"].to(device)\n",
        "            input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n",
        "            output =  model(input_id, mask)\n",
        "            out.append(output.tolist())\n",
        "                                               \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM4iSKRYXG27"
      },
      "outputs": [],
      "source": [
        "#evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6DEGOOEXG28"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size = 2)\n",
        "    criterion = nn.MSELoss()\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_loss_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_labels in tqdm(test_dataloader):\n",
        "\n",
        "            test_labels = test_labels.to(device)\n",
        "            mask = test_input['attention_mask'].to(device)\n",
        "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            loss = criterion(output, test_labels)\n",
        "            total_loss_test += loss\n",
        "    \n",
        "    print(f'Test Loss: {total_loss_test / len(test_data): .3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b0a83cb726a542fca7b5c68ca061dbc1"
          ]
        },
        "id": "VguTVZe5XG28",
        "outputId": "03c6816b-2338-4607-b459-b09685bb829f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0a83cb726a542fca7b5c68ca061dbc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1956 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = FeedbackModel()\n",
        "\n",
        "EPOCHS = 30\n",
        "train(model, train_df, df_val, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6taRJIHXG28"
      },
      "outputs": [],
      "source": [
        "evaluate(model, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5QrxYyPXG28"
      },
      "outputs": [],
      "source": [
        "torch.save(model.cpu().state_dict(), \"BERT_epoch_30.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkJELffPXG29"
      },
      "outputs": [],
      "source": [
        "prediction = predict(model, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbbnI6dmXG29"
      },
      "outputs": [],
      "source": [
        "######## Implementing deberta ####################################################################################################################################33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR2A9rzdXmb6"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.20\n",
        "!pip install tensorflow-addons\n",
        "!pip install iterstrat\n",
        "!pip install imbalanced-learn\n",
        "import transformers\n",
        "print(f\"trainsformer version {transformers.__version__}\")\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
        "model = TFDebertaModel.from_pretrained('microsoft/deberta-base')\n",
        "!pip install iterative-stratification\n",
        "import sys\n",
        "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UiIYbBRXmlo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, TFDebertaV2Model, AutoConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow_addons as tfa\n",
        "import sys\n",
        "sys.path.append('../input/iterativestratification')\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "import tensorflow_addons as tfa\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m9dKkYoXmn0"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4NqhrbvXmqc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define and initialize the distribution strategy\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PDFGwCdXmsv"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8*strategy.num_replicas_in_sync\n",
        "BUFFER_SIZE = 3200\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "SEQ_LEN = 512\n",
        "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
        "FOLD_NUM = 4\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
        "TARGET_COLS = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp1ODNVDXmu3"
      },
      "outputs": [],
      "source": [
        "def preprocess(df, tokenizer):\n",
        "    inputs, labels = np.array(df[\"full_text\"]), np.array(df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]])\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    for x in tqdm(inputs):\n",
        "        tokens = tokenizer(x, padding=\"max_length\", truncation=True, max_length=SEQ_LEN, return_tensors=\"np\")\n",
        "        ids = tokens[\"input_ids\"]\n",
        "        mask = tokens[\"attention_mask\"]\n",
        "        input_ids.append(ids)\n",
        "        attention_mask.append(mask)\n",
        "    input_ids = np.array(input_ids).squeeze()\n",
        "    attention_mask = np.array(attention_mask).squeeze()\n",
        "    return input_ids, attention_mask, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guQ-VkMBX_9L"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"/content/train.csv\")\n",
        "kf = MultilabelStratifiedKFold(n_splits=FOLD_NUM, shuffle=True, random_state=123)\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(df, df[TARGET_COLS])):\n",
        "    df.loc[test_idx, \"fold\"] = int(fold)\n",
        "df[\"fold\"] = df[\"fold\"].astype(int)\n",
        "# df.to_csv(\"train_fold.csv\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceCMHkLsX__I"
      },
      "outputs": [],
      "source": [
        "def get_train_dataset(ids, mask, y):\n",
        "    x = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\": tf.constant(ids, dtype=\"int32\"),\n",
        "        \"attention_mask\": tf.constant(mask, dtype=\"int32\")\n",
        "    })\n",
        "    y = tf.data.Dataset.from_tensor_slices(y)\n",
        "    data = tf.data.Dataset.zip((x, y))\n",
        "    data = data.repeat()\n",
        "    data = data.shuffle(BUFFER_SIZE)\n",
        "    data = data.batch(BATCH_SIZE)\n",
        "    data = data.prefetch(AUTO)\n",
        "    return data\n",
        "\n",
        "def get_val_dataset(ids, mask, y):\n",
        "    x = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\": tf.constant(ids, dtype=\"int32\"),\n",
        "        \"attention_mask\": tf.constant(mask, dtype=\"int32\")\n",
        "    })\n",
        "    y = tf.data.Dataset.from_tensor_slices(y)\n",
        "    data = tf.data.Dataset.zip((x, y))\n",
        "    data = data.repeat()\n",
        "    data = data.batch(BATCH_SIZE)\n",
        "    data = data.prefetch(AUTO)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W1MzedwYACe"
      },
      "outputs": [],
      "source": [
        "class AttentionPool(layers.Layer):\n",
        "    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n",
        "        super().__init__()\n",
        "        self.num_hidden_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hiddendim_fc = hiddendim_fc\n",
        "        self.dropout = layers.Dropout(0.0)\n",
        "        \n",
        "        self.q = tf.Variable(initial_value=keras.initializers.GlorotNormal()(shape=(1, self.hidden_size), dtype=tf.float32), name=\"attention_pool_q\")\n",
        "        self.w_h = tf.Variable(initial_value=keras.initializers.GlorotNormal()(shape=(self.hidden_size, self.hiddendim_fc), dtype=tf.float32), name=\"attention_pool_wh\")\n",
        "    \n",
        "    def call(self, all_hidden_states):\n",
        "        # use CLS token\n",
        "        hidden_states  = tf.stack([all_hidden_states[i][:, 0] for i in range(1, self.num_hidden_layers+1)], axis=-1)\n",
        "        hidden_states = tf.reshape(hidden_states, (-1, self.num_hidden_layers, self.hidden_size))\n",
        "        out = self.attention(hidden_states)\n",
        "        out = self.dropout(out)\n",
        "        return out \n",
        "    \n",
        "    def attention(self, h):\n",
        "        v = tf.squeeze(tf.matmul(self.q, tf.transpose(h, perm=(0, 2, 1))), axis=1)\n",
        "        v = tf.nn.softmax(v, axis=-1)\n",
        "        v_temp = tf.transpose(tf.matmul(tf.expand_dims(v, axis=1), h), perm=(0, 2, 1))\n",
        "        v = tf.squeeze(tf.matmul(tf.transpose(self.w_h, perm=(1, 0)), v_temp), axis=2)\n",
        "        return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVBDMdtPXmyV"
      },
      "outputs": [],
      "source": [
        "class MeanPool(keras.layers.Layer):\n",
        "    def call(self, x, mask=None):\n",
        "        broad_mask = tf.cast(tf.expand_dims(mask, -1), \"float32\")\n",
        "        # [batch, maxlen, hidden_state]\n",
        "        x = tf.math.reduce_sum( x * broad_mask, axis=1)\n",
        "        x = x / tf.math.maximum(tf.reduce_sum(broad_mask, axis=1), tf.constant([1e-9]))\n",
        "        return x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMEPSInJYjs9"
      },
      "outputs": [],
      "source": [
        "from transformers.tf_utils import shape_list\n",
        "\n",
        "def take_along_axis(x, indices, gather_axis):\n",
        "    # Only a valid port of np.take_along_axis when the gather axis is -1\n",
        "\n",
        "    # TPU + gathers and reshapes don't go along well -- see https://github.com/huggingface/transformers/issues/18239\n",
        "    if isinstance(tf.distribute.get_strategy(), tf.distribute.TPUStrategy):\n",
        "        # [B, S, P] -> [B, S, P, D]\n",
        "        one_hot_indices = tf.one_hot(indices, depth=x.shape[-1], dtype=x.dtype)\n",
        "\n",
        "        # if we ignore the first two dims, this is equivalent to multiplying a matrix (one hot) by a vector (x)\n",
        "        # grossly abusing notation: [B, S, P, D] . [B, S, D] = [B, S, P]\n",
        "        gathered = tf.einsum(\"ijkl,ijl->ijk\", one_hot_indices, x)\n",
        "\n",
        "    # GPUs, on the other hand, prefer gathers instead of large one-hot+matmuls\n",
        "    else:\n",
        "        gathered = tf.gather(x, indices, batch_dims=2)\n",
        "\n",
        "    return gathered\n",
        "\n",
        "\n",
        "transformers.models.deberta_v2.modeling_tf_deberta_v2.take_along_axis = take_along_axis\n",
        "\n",
        "class TFDebertaV2StableDropout(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Optimized dropout module for stabilizing the training\n",
        "    Args:\n",
        "        drop_prob (float): the dropout probabilities\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    @tf.custom_gradient\n",
        "    def xdropout(self, inputs):\n",
        "        \"\"\"\n",
        "        Applies dropout to the inputs, as vanilla dropout, but also scales the remaining elements up by 1/drop_prob.\n",
        "        \"\"\"\n",
        "        mask = tf.cast(\n",
        "            1\n",
        "            - tf.compat.v1.distributions.Bernoulli(probs=1.0 - self.drop_prob).sample(sample_shape=shape_list(inputs)),\n",
        "            tf.bool,\n",
        "        )\n",
        "        scale = tf.convert_to_tensor(1.0 / (1 - self.drop_prob), dtype=tf.float32)\n",
        "        if self.drop_prob > 0:\n",
        "            inputs = tf.where(mask, 0.0, inputs) * scale\n",
        "\n",
        "        def grad(upstream):\n",
        "            if self.drop_prob > 0:\n",
        "                return tf.where(mask, 0.0, upstream) * scale\n",
        "            else:\n",
        "                return upstream\n",
        "\n",
        "        return inputs, grad\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, training: tf.Tensor = False):\n",
        "        if training:\n",
        "            return self.xdropout(inputs)\n",
        "        return inputs\n",
        "\n",
        "transformers.models.deberta_v2.modeling_tf_deberta_v2.TFDebertaV2StableDropout = TFDebertaV2StableDropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RhJSIKlYj7t"
      },
      "outputs": [],
      "source": [
        "def build_model(trainable=True):\n",
        "    input1 = keras.Input(shape=(None,), batch_size=BATCH_SIZE, dtype=\"int32\", name=\"input_ids\")\n",
        "    input2 = keras.Input(shape=(None,), batch_size=BATCH_SIZE, dtype=\"int32\", name=\"attention_mask\")\n",
        "    \n",
        "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "    config.attention_probs_dropout_prob = 0.0\n",
        "    config.hidden_dropout_prob = 0.0\n",
        "    config.update({\"output_hidden_states\": True})\n",
        "    \n",
        "    base_model = TFDebertaV2Model.from_pretrained(\n",
        "        MODEL_NAME, \n",
        "        config=config,\n",
        "    )\n",
        "    # Re-initialize last layer \n",
        "    REINIT_LAYER = 1\n",
        "    for layer in base_model.deberta.encoder.layer[-REINIT_LAYER:]:\n",
        "        for module in layer.submodules:\n",
        "            if isinstance(module, layers.Dense):\n",
        "                module.kernel.assign(keras.initializers.GlorotUniform()(shape=module.kernel.shape, dtype=module.kernel.dtype))\n",
        "            elif isinstance(module, layers.LayerNormalization):\n",
        "                module.beta.assign(keras.initializers.Zeros()(shape=module.beta.shape, dtype=module.beta.dtype))\n",
        "                module.gamma.assign(keras.initializers.Ones()(shape=module.gamma.shape, dtype=module.gamma.dtype))\n",
        "            \n",
        "    base_model.trainable = trainable\n",
        "    base_outputs = base_model.deberta({\"input_ids\": input1,\n",
        "                              \"attention_mask\": input2})\n",
        "    all_hidden_states = tf.stack(base_outputs[1])\n",
        "    hiddendim_fc = 512\n",
        "    pooler = AttentionPool(config.num_hidden_layers, config.hidden_size, hiddendim_fc)\n",
        "    attention_pooling_embeddings = pooler(all_hidden_states)\n",
        "    outputs = layers.Dense(6)(attention_pooling_embeddings)\n",
        "    model = keras.Model(inputs={\"input_ids\": input1,\"attention_mask\": input2}, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO5IPQmQY88R"
      },
      "outputs": [],
      "source": [
        "# Multi-optimizers\n",
        "def get_optimizers(model, base_lr, head_lr, train_num):\n",
        "    \n",
        "    layer_list = [model.get_layer(\"deberta\").embeddings] + model.get_layer(\"deberta\").encoder.layer\n",
        "    layer_list.reverse()\n",
        "    base_schedule = [keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=base_lr*0.9**i,\n",
        "                                                        decay_steps=train_num//BATCH_SIZE,\n",
        "                                                        decay_rate=0.9) for i in range(len(layer_list))]\n",
        "    base_optimizers = [keras.optimizers.Adam(learning_rate=lr_schedule) for lr_schedule in base_schedule]\n",
        "    \n",
        "    head_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=head_lr,\n",
        "                                                                 decay_steps=train_num//BATCH_SIZE,\n",
        "                                                                 decay_rate=0.9)\n",
        "    head_optimizers = keras.optimizers.Adam(learning_rate=head_schedule)\n",
        "    # Get head layers\n",
        "    idx = 3\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if layer.name == \"deberta\":\n",
        "            idx = i\n",
        "    optimizers = tfa.optimizers.MultiOptimizer([(head_optimizers, model.layers[idx+1:])] + list(zip(base_optimizers, layer_list)))\n",
        "    return optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSwYCC-bY9GL"
      },
      "outputs": [],
      "source": [
        "tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://10.0.0.2:8470')\n",
        "tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu_resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33uD-pNCZF6R"
      },
      "outputs": [],
      "source": [
        "historys = []\n",
        "\n",
        "for fold in range(FOLD_NUM):\n",
        "    # Initialize tpu\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    print(f\"{'#'*25}\\nFold {fold}\")\n",
        "    train_df = df[df[\"fold\"]!=fold].reset_index(drop=True)\n",
        "    val_df = df[df[\"fold\"]==fold].reset_index(drop=True)\n",
        "    train_ids, train_mask, train_y = preprocess(train_df, tokenizer) \n",
        "    val_ids, val_mask, val_y = preprocess(val_df, tokenizer)                                    \n",
        "    train_dataset = get_train_dataset(train_ids, train_mask, train_y)\n",
        "    val_dataset = get_val_dataset(val_ids, val_mask, val_y)\n",
        "    TRAIN_NUM = int(len(train_ids))\n",
        "    VAL_NUM = int(len(val_ids))\n",
        "    \n",
        "    tf.keras.backend.clear_session()\n",
        "    with strategy.scope():\n",
        "        model = build_model(trainable=True)\n",
        "        model.compile(loss=keras.losses.MeanSquaredError(),\n",
        "                      optimizer=get_optimizers(model, base_lr=1e-5, head_lr=1e-3, train_num=TRAIN_NUM),\n",
        "                     metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "    # callbacks\n",
        "    save_locally = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "    CHECKPOINT_PATH = f\"./deberta_model_fold{fold}.h5\"\n",
        "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH,\n",
        "                                                                monitor='val_loss',\n",
        "                                                                options=save_locally,\n",
        "                                                                save_best_only=True,\n",
        "                                                                save_weights_only=True,\n",
        "                                                                verbose=1)\n",
        "\n",
        "    earlystop_callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                      patience=6,\n",
        "                                                      verbose=1)\n",
        " \n",
        "    callbacks = [\n",
        "        model_checkpoint_callback,\n",
        "        earlystop_callback,\n",
        "    ]\n",
        "    \n",
        "    history = model.fit(train_dataset,\n",
        "                            validation_data=val_dataset,\n",
        "                            steps_per_epoch=TRAIN_NUM // BATCH_SIZE,\n",
        "                            validation_steps=VAL_NUM // BATCH_SIZE,\n",
        "                            callbacks=callbacks,\n",
        "                            epochs=30,\n",
        "                            verbose=1,\n",
        "                           )\n",
        "    historys.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m01TaN5-ZKwP"
      },
      "outputs": [],
      "source": [
        "def plot_history(historys):\n",
        "    for id, history in enumerate(historys):\n",
        "        loss = history.history['root_mean_squared_error']\n",
        "        min_loss = round(np.min(loss), 6)\n",
        "        epoch = range(len(loss))\n",
        "        plt.plot(epoch, loss, label=f\"fold {id}: {min_loss}\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Train rmse\")\n",
        "    plt.figure()\n",
        "    for id, history in enumerate(historys):\n",
        "        val_rmse = history.history['val_root_mean_squared_error']\n",
        "        min_val_rmse = round(np.min(val_rmse), 6)\n",
        "        epoch = range(len(val_rmse))\n",
        "        plt.plot(epoch, val_rmse, label=f\"fold {id}: {min_val_rmse}\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Val rmse\")\n",
        "\n",
        "\n",
        "plot_history(historys)\n",
        "\n",
        "def get_scores(historys):\n",
        "    scores = []\n",
        "    for id, history in enumerate(historys):\n",
        "        val_rmse = history.history['val_root_mean_squared_error']\n",
        "        min_val_rmse = np.min(val_rmse)\n",
        "        scores.append(min_val_rmse)\n",
        "    return scores\n",
        "\n",
        "scores = get_scores(historys)\n",
        "print(f\"\\nAll scores of K fold validation: {scores}\\n\")\n",
        "print(f\"Mean score: {np.mean(scores)}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DiCXng8XG29"
      },
      "source": [
        "# Conclusion and Future Direction\n",
        "\n",
        "DeBERTa's future development is likely to be directed on enhancing its performance on a variety of NLP tasks, including those that call for greater contextual knowledge and deductive reasoning skills. Here are some potential directions for DeBERTa going forward:\n",
        "\n",
        "-> Multimodal Language Understanding: To better comprehend the context of natural language, DeBERTa can be extended to handle multimodal inputs, such as text and visuals. This has a number of uses, including captioning pictures and visual question answering.\n",
        "\n",
        "-> Few-Shot Learning: DeBERTa can be further enhanced to perform better on tasks requiring very sparse samples for the model to learn from. This can be accomplished by utilising strategies like transfer learning, meta-learning, and more.\n",
        "\n",
        "-> DeBERTa can be expanded to learn from a stream of data in a perpetual learning environment, where the model must adapt to new tasks and ideas without forgetting the ones it has already learned. This can be helpful in a variety of real-world situations where the data is ever-changing.\n",
        "\n",
        "-> DeBERTa can be customised to include privacy-preserving methods like federated learning and differential privacy to safeguard user privacy and secure sensitive data.\n",
        "\n",
        "Overall, DeBERTa's future development is probably going to be centred on enhancing its performance on a variety of NLP tasks, as well as expanding its capacity to handle multimodal inputs, few-shot learning, continuous learning, and privacy-preserving learning.\n",
        "\n",
        "Using the the Kfold the accuracy of the DeBERTa model has increased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ8eg1RGXG29"
      },
      "source": [
        "# References:\n",
        "\n",
        "[1]:  Official DeBERTa Github repository: https://github.com/microsoft/DeBERTa.\n",
        "\n",
        "[2]:  Microsoft Research Asia. \"Microsoft Research Asia achieves state-of-the-art performance on natural language processing benchmarks with    DeBERTa.\" Microsoft News Center, 2020\n",
        "\n",
        "[3]:  Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Xiaodong Liu. \"DeBERTa: Bridging the Gap between RoBERTa and GPT-2.\" In arXiv preprint arXiv:2008.05663, 2020."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}